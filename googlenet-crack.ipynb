{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b\"In file included from /tmp/try_flags_rsu773ye.c:3:0:\\n/usr/include/stdio.h:365:45: error: expected initializer before '__THROWNL'\\n       const char *__restrict __format, ...) __THROWNL;\\n                                             ^~~~~~~~~\\n/usr/include/stdio.h:380:26: error: expected initializer before '__THROWNL'\\n        _G_va_list __arg) __THROWNL;\\n                          ^~~~~~~~~\\n/usr/include/stdio.h:388:6: error: expected initializer before '__THROWNL'\\n      __THROWNL __attribute__ ((__format__ (__printf__, 3, 4)));\\n      ^~~~~~~~~\\n/usr/include/stdio.h:392:6: error: expected initializer before '__THROWNL'\\n      __THROWNL __attribute__ ((__format__ (__printf__, 3, 0)));\\n      ^~~~~~~~~\\n/usr/include/stdio.h:401:6: error: expected initializer before '__THROWNL'\\n      __THROWNL __attribute__ ((__format__ (__printf__, 2, 0))) __wur;\\n      ^~~~~~~~~\\n/usr/include/stdio.h:404:6: error: expected initializer before '__THROWNL'\\n      __THROWNL __attribute__ ((__format__ (__printf__, 2, 3))) __wur;\\n      ^~~~~~~~~\\n/usr/include/stdio.h:407:6: error: expected initializer before '__THROWNL'\\n      __THROWNL __attribute__ ((__format__ (__printf__, 2, 3))) __wur;\\n      ^~~~~~~~~\\nIn file included from /tmp/try_flags_rsu773ye.c:3:0:\\n/usr/include/stdio.h:900:6: error: expected initializer before '__THROWNL'\\n      __THROWNL __attribute__ ((__format__ (__printf__, 2, 3)));\\n      ^~~~~~~~~\\n/usr/include/stdio.h:904:6: error: expected initializer before '__THROWNL'\\n      __THROWNL __attribute__ ((__format__ (__printf__, 2, 0)));\\n      ^~~~~~~~~\\n\"\n",
      "Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:65:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os; os.environ['KERAS_BACKEND'] = 'theano'\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=cuda,floatX=float32\"\n",
    "import theano\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation, concatenate\n",
    "from keras.datasets import cifar10\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from custom_layers.googlenet_custom_layers import LRN, PoolHelper\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from custom_layers.scale_layer import Scale\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def googlenet_model(img_rows, img_cols, channel=1, num_classes=None):\n",
    "    \"\"\"\n",
    "    GoogLeNet a.k.a. Inception v1 for Keras\n",
    "    Model Schema is based on \n",
    "    https://gist.github.com/joelouismarino/a2ede9ab3928f999575423b9887abd14\n",
    "    ImageNet Pretrained Weights \n",
    "    https://drive.google.com/open?id=0B319laiAPjU3RE1maU9MMlh2dnc\n",
    "    Blog Post: \n",
    "    http://joelouismarino.github.io/blog_posts/blog_googlenet_keras.html\n",
    "    Parameters:\n",
    "      img_rows, img_cols - resolution of inputs\n",
    "      channel - 1 for grayscale, 3 for color \n",
    "      num_classes - number of class labels for our classification task\n",
    "    \"\"\"\n",
    "    \n",
    "    input = Input(shape=(channel, img_rows, img_cols))\n",
    "    conv1_7x7_s2 = Convolution2D(64,7,7,subsample=(2,2),border_mode='same',activation='relu',name='conv1/7x7_s2',W_regularizer=l2(0.0002))(input)\n",
    "    conv1_zero_pad = ZeroPadding2D(padding=(1, 1))(conv1_7x7_s2)\n",
    "    pool1_helper = PoolHelper()(conv1_zero_pad)\n",
    "    pool1_3x3_s2 = MaxPooling2D(pool_size=(3,3),strides=(2,2),border_mode='valid',name='pool1/3x3_s2')(pool1_helper)\n",
    "    pool1_norm1 = LRN(name='pool1/norm1')(pool1_3x3_s2)\n",
    "    conv2_3x3_reduce = Convolution2D(64,1,1,border_mode='same',activation='relu',name='conv2/3x3_reduce',W_regularizer=l2(0.0002))(pool1_norm1)\n",
    "    conv2_3x3 = Convolution2D(192,3,3,border_mode='same',activation='relu',name='conv2/3x3',W_regularizer=l2(0.0002))(conv2_3x3_reduce)\n",
    "    conv2_norm2 = LRN(name='conv2/norm2')(conv2_3x3)\n",
    "    conv2_zero_pad = ZeroPadding2D(padding=(1, 1))(conv2_norm2)\n",
    "    pool2_helper = PoolHelper()(conv2_zero_pad)\n",
    "    pool2_3x3_s2 = MaxPooling2D(pool_size=(3,3),strides=(2,2),border_mode='valid',name='pool2/3x3_s2')(pool2_helper)\n",
    "    \n",
    "    inception_3a_1x1 = Convolution2D(64,1,1,border_mode='same',activation='relu',name='inception_3a/1x1',W_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3_reduce = Convolution2D(96,1,1,border_mode='same',activation='relu',name='inception_3a/3x3_reduce',W_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_3x3 = Convolution2D(128,3,3,border_mode='same',activation='relu',name='inception_3a/3x3',W_regularizer=l2(0.0002))(inception_3a_3x3_reduce)\n",
    "    inception_3a_5x5_reduce = Convolution2D(16,1,1,border_mode='same',activation='relu',name='inception_3a/5x5_reduce',W_regularizer=l2(0.0002))(pool2_3x3_s2)\n",
    "    inception_3a_5x5 = Convolution2D(32,5,5,border_mode='same',activation='relu',name='inception_3a/5x5',W_regularizer=l2(0.0002))(inception_3a_5x5_reduce)\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_3a/pool')(pool2_3x3_s2)\n",
    "    inception_3a_pool_proj = Convolution2D(32,1,1,border_mode='same',activation='relu',name='inception_3a/pool_proj',W_regularizer=l2(0.0002))(inception_3a_pool)\n",
    "    inception_3a_output = merge([inception_3a_1x1,inception_3a_3x3,inception_3a_5x5,inception_3a_pool_proj],mode='concat',concat_axis=1,name='inception_3a/output')\n",
    "    \n",
    "    inception_3b_1x1 = Convolution2D(128,1,1,border_mode='same',activation='relu',name='inception_3b/1x1',W_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3_reduce = Convolution2D(128,1,1,border_mode='same',activation='relu',name='inception_3b/3x3_reduce',W_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_3x3 = Convolution2D(192,3,3,border_mode='same',activation='relu',name='inception_3b/3x3',W_regularizer=l2(0.0002))(inception_3b_3x3_reduce)\n",
    "    inception_3b_5x5_reduce = Convolution2D(32,1,1,border_mode='same',activation='relu',name='inception_3b/5x5_reduce',W_regularizer=l2(0.0002))(inception_3a_output)\n",
    "    inception_3b_5x5 = Convolution2D(96,5,5,border_mode='same',activation='relu',name='inception_3b/5x5',W_regularizer=l2(0.0002))(inception_3b_5x5_reduce)\n",
    "    inception_3b_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_3b/pool')(inception_3a_output)\n",
    "    inception_3b_pool_proj = Convolution2D(64,1,1,border_mode='same',activation='relu',name='inception_3b/pool_proj',W_regularizer=l2(0.0002))(inception_3b_pool)\n",
    "    inception_3b_output = merge([inception_3b_1x1,inception_3b_3x3,inception_3b_5x5,inception_3b_pool_proj],mode='concat',concat_axis=1,name='inception_3b/output')\n",
    "    \n",
    "    inception_3b_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_3b_output)\n",
    "    pool3_helper = PoolHelper()(inception_3b_output_zero_pad)\n",
    "    pool3_3x3_s2 = MaxPooling2D(pool_size=(3,3),strides=(2,2),border_mode='valid',name='pool3/3x3_s2')(pool3_helper)\n",
    "    \n",
    "    inception_4a_1x1 = Convolution2D(192,1,1,border_mode='same',activation='relu',name='inception_4a/1x1',W_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3_reduce = Convolution2D(96,1,1,border_mode='same',activation='relu',name='inception_4a/3x3_reduce',W_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_3x3 = Convolution2D(208,3,3,border_mode='same',activation='relu',name='inception_4a/3x3',W_regularizer=l2(0.0002))(inception_4a_3x3_reduce)\n",
    "    inception_4a_5x5_reduce = Convolution2D(16,1,1,border_mode='same',activation='relu',name='inception_4a/5x5_reduce',W_regularizer=l2(0.0002))(pool3_3x3_s2)\n",
    "    inception_4a_5x5 = Convolution2D(48,5,5,border_mode='same',activation='relu',name='inception_4a/5x5',W_regularizer=l2(0.0002))(inception_4a_5x5_reduce)\n",
    "    inception_4a_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_4a/pool')(pool3_3x3_s2)\n",
    "    inception_4a_pool_proj = Convolution2D(64,1,1,border_mode='same',activation='relu',name='inception_4a/pool_proj',W_regularizer=l2(0.0002))(inception_4a_pool)\n",
    "    inception_4a_output = merge([inception_4a_1x1,inception_4a_3x3,inception_4a_5x5,inception_4a_pool_proj],mode='concat',concat_axis=1,name='inception_4a/output')\n",
    "    \n",
    "    loss1_ave_pool = AveragePooling2D(pool_size=(5,5),strides=(3,3),name='loss1/ave_pool')(inception_4a_output)\n",
    "    loss1_conv = Convolution2D(128,1,1,border_mode='same',activation='relu',name='loss1/conv',W_regularizer=l2(0.0002))(loss1_ave_pool)\n",
    "    loss1_flat = Flatten()(loss1_conv)\n",
    "    loss1_fc = Dense(1024,activation='relu',name='loss1/fc',W_regularizer=l2(0.0002))(loss1_flat)\n",
    "    loss1_drop_fc = Dropout(0.7)(loss1_fc)\n",
    "    loss1_classifier = Dense(1000,name='loss1/classifier',W_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act = Activation('softmax')(loss1_classifier)\n",
    "    \n",
    "    inception_4b_1x1 = Convolution2D(160,1,1,border_mode='same',activation='relu',name='inception_4b/1x1',W_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3_reduce = Convolution2D(112,1,1,border_mode='same',activation='relu',name='inception_4b/3x3_reduce',W_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_3x3 = Convolution2D(224,3,3,border_mode='same',activation='relu',name='inception_4b/3x3',W_regularizer=l2(0.0002))(inception_4b_3x3_reduce)\n",
    "    inception_4b_5x5_reduce = Convolution2D(24,1,1,border_mode='same',activation='relu',name='inception_4b/5x5_reduce',W_regularizer=l2(0.0002))(inception_4a_output)\n",
    "    inception_4b_5x5 = Convolution2D(64,5,5,border_mode='same',activation='relu',name='inception_4b/5x5',W_regularizer=l2(0.0002))(inception_4b_5x5_reduce)\n",
    "    inception_4b_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_4b/pool')(inception_4a_output)\n",
    "    inception_4b_pool_proj = Convolution2D(64,1,1,border_mode='same',activation='relu',name='inception_4b/pool_proj',W_regularizer=l2(0.0002))(inception_4b_pool)\n",
    "    inception_4b_output = merge([inception_4b_1x1,inception_4b_3x3,inception_4b_5x5,inception_4b_pool_proj],mode='concat',concat_axis=1,name='inception_4b_output')\n",
    "    \n",
    "    inception_4c_1x1 = Convolution2D(128,1,1,border_mode='same',activation='relu',name='inception_4c/1x1',W_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3_reduce = Convolution2D(128,1,1,border_mode='same',activation='relu',name='inception_4c/3x3_reduce',W_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_3x3 = Convolution2D(256,3,3,border_mode='same',activation='relu',name='inception_4c/3x3',W_regularizer=l2(0.0002))(inception_4c_3x3_reduce)\n",
    "    inception_4c_5x5_reduce = Convolution2D(24,1,1,border_mode='same',activation='relu',name='inception_4c/5x5_reduce',W_regularizer=l2(0.0002))(inception_4b_output)\n",
    "    inception_4c_5x5 = Convolution2D(64,5,5,border_mode='same',activation='relu',name='inception_4c/5x5',W_regularizer=l2(0.0002))(inception_4c_5x5_reduce)\n",
    "    inception_4c_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_4c/pool')(inception_4b_output)\n",
    "    inception_4c_pool_proj = Convolution2D(64,1,1,border_mode='same',activation='relu',name='inception_4c/pool_proj',W_regularizer=l2(0.0002))(inception_4c_pool)\n",
    "    inception_4c_output = merge([inception_4c_1x1,inception_4c_3x3,inception_4c_5x5,inception_4c_pool_proj],mode='concat',concat_axis=1,name='inception_4c/output')\n",
    "    \n",
    "    inception_4d_1x1 = Convolution2D(112,1,1,border_mode='same',activation='relu',name='inception_4d/1x1',W_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3_reduce = Convolution2D(144,1,1,border_mode='same',activation='relu',name='inception_4d/3x3_reduce',W_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_3x3 = Convolution2D(288,3,3,border_mode='same',activation='relu',name='inception_4d/3x3',W_regularizer=l2(0.0002))(inception_4d_3x3_reduce)\n",
    "    inception_4d_5x5_reduce = Convolution2D(32,1,1,border_mode='same',activation='relu',name='inception_4d/5x5_reduce',W_regularizer=l2(0.0002))(inception_4c_output)\n",
    "    inception_4d_5x5 = Convolution2D(64,5,5,border_mode='same',activation='relu',name='inception_4d/5x5',W_regularizer=l2(0.0002))(inception_4d_5x5_reduce)\n",
    "    inception_4d_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_4d/pool')(inception_4c_output)\n",
    "    inception_4d_pool_proj = Convolution2D(64,1,1,border_mode='same',activation='relu',name='inception_4d/pool_proj',W_regularizer=l2(0.0002))(inception_4d_pool)\n",
    "    inception_4d_output = merge([inception_4d_1x1,inception_4d_3x3,inception_4d_5x5,inception_4d_pool_proj],mode='concat',concat_axis=1,name='inception_4d/output')\n",
    "    \n",
    "    loss2_ave_pool = AveragePooling2D(pool_size=(5,5),strides=(3,3),name='loss2/ave_pool')(inception_4d_output)\n",
    "    loss2_conv = Convolution2D(128,1,1,border_mode='same',activation='relu',name='loss2/conv',W_regularizer=l2(0.0002))(loss2_ave_pool)\n",
    "    loss2_flat = Flatten()(loss2_conv)\n",
    "    loss2_fc = Dense(1024,activation='relu',name='loss2/fc',W_regularizer=l2(0.0002))(loss2_flat)\n",
    "    loss2_drop_fc = Dropout(0.7)(loss2_fc)\n",
    "    loss2_classifier = Dense(1000,name='loss2/classifier',W_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act = Activation('softmax')(loss2_classifier)\n",
    "    \n",
    "    inception_4e_1x1 = Convolution2D(256,1,1,border_mode='same',activation='relu',name='inception_4e/1x1',W_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3_reduce = Convolution2D(160,1,1,border_mode='same',activation='relu',name='inception_4e/3x3_reduce',W_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_3x3 = Convolution2D(320,3,3,border_mode='same',activation='relu',name='inception_4e/3x3',W_regularizer=l2(0.0002))(inception_4e_3x3_reduce)\n",
    "    inception_4e_5x5_reduce = Convolution2D(32,1,1,border_mode='same',activation='relu',name='inception_4e/5x5_reduce',W_regularizer=l2(0.0002))(inception_4d_output)\n",
    "    inception_4e_5x5 = Convolution2D(128,5,5,border_mode='same',activation='relu',name='inception_4e/5x5',W_regularizer=l2(0.0002))(inception_4e_5x5_reduce)\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_4e/pool')(inception_4d_output)\n",
    "    inception_4e_pool_proj = Convolution2D(128,1,1,border_mode='same',activation='relu',name='inception_4e/pool_proj',W_regularizer=l2(0.0002))(inception_4e_pool)\n",
    "    inception_4e_output = merge([inception_4e_1x1,inception_4e_3x3,inception_4e_5x5,inception_4e_pool_proj],mode='concat',concat_axis=1,name='inception_4e/output')\n",
    "    \n",
    "    inception_4e_output_zero_pad = ZeroPadding2D(padding=(1, 1))(inception_4e_output)\n",
    "    pool4_helper = PoolHelper()(inception_4e_output_zero_pad)\n",
    "    pool4_3x3_s2 = MaxPooling2D(pool_size=(3,3),strides=(2,2),border_mode='valid',name='pool4/3x3_s2')(pool4_helper)\n",
    "    \n",
    "    inception_5a_1x1 = Convolution2D(256,1,1,border_mode='same',activation='relu',name='inception_5a/1x1',W_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3_reduce = Convolution2D(160,1,1,border_mode='same',activation='relu',name='inception_5a/3x3_reduce',W_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_3x3 = Convolution2D(320,3,3,border_mode='same',activation='relu',name='inception_5a/3x3',W_regularizer=l2(0.0002))(inception_5a_3x3_reduce)\n",
    "    inception_5a_5x5_reduce = Convolution2D(32,1,1,border_mode='same',activation='relu',name='inception_5a/5x5_reduce',W_regularizer=l2(0.0002))(pool4_3x3_s2)\n",
    "    inception_5a_5x5 = Convolution2D(128,5,5,border_mode='same',activation='relu',name='inception_5a/5x5',W_regularizer=l2(0.0002))(inception_5a_5x5_reduce)\n",
    "    inception_5a_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_5a/pool')(pool4_3x3_s2)\n",
    "    inception_5a_pool_proj = Convolution2D(128,1,1,border_mode='same',activation='relu',name='inception_5a/pool_proj',W_regularizer=l2(0.0002))(inception_5a_pool)\n",
    "    inception_5a_output = merge([inception_5a_1x1,inception_5a_3x3,inception_5a_5x5,inception_5a_pool_proj],mode='concat',concat_axis=1,name='inception_5a/output')\n",
    "    \n",
    "    inception_5b_1x1 = Convolution2D(384,1,1,border_mode='same',activation='relu',name='inception_5b/1x1',W_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3_reduce = Convolution2D(192,1,1,border_mode='same',activation='relu',name='inception_5b/3x3_reduce',W_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_3x3 = Convolution2D(384,3,3,border_mode='same',activation='relu',name='inception_5b/3x3',W_regularizer=l2(0.0002))(inception_5b_3x3_reduce)\n",
    "    inception_5b_5x5_reduce = Convolution2D(48,1,1,border_mode='same',activation='relu',name='inception_5b/5x5_reduce',W_regularizer=l2(0.0002))(inception_5a_output)\n",
    "    inception_5b_5x5 = Convolution2D(128,5,5,border_mode='same',activation='relu',name='inception_5b/5x5',W_regularizer=l2(0.0002))(inception_5b_5x5_reduce)\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_5b/pool')(inception_5a_output)\n",
    "    inception_5b_pool_proj = Convolution2D(128,1,1,border_mode='same',activation='relu',name='inception_5b/pool_proj',W_regularizer=l2(0.0002))(inception_5b_pool)\n",
    "    inception_5b_output = merge([inception_5b_1x1,inception_5b_3x3,inception_5b_5x5,inception_5b_pool_proj],mode='concat',concat_axis=1,name='inception_5b/output')\n",
    "    \n",
    "    pool5_7x7_s1 = AveragePooling2D(pool_size=(7,7),strides=(1,1),name='pool5/7x7_s2')(inception_5b_output)\n",
    "    loss3_flat = Flatten()(pool5_7x7_s1)\n",
    "    pool5_drop_7x7_s1 = Dropout(0.4)(loss3_flat)\n",
    "    loss3_classifier = Dense(1000,name='loss3/classifier',W_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act = Activation('softmax',name='prob')(loss3_classifier)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(input=input, output=[loss1_classifier_act,loss2_classifier_act,loss3_classifier_act])\n",
    "    \n",
    "    # Load ImageNet pre-trained data \n",
    "    model.load_weights('/home/sahand/Projects/Kaggle-dog-breeds-identification/imagenet_models/googlenet_weights.h5')\n",
    "    \n",
    "    # Truncate and replace softmax layer for transfer learning\n",
    "    # Cannot use model.layers.pop() since model is not of Sequential() type\n",
    "    # The method below works since pre-trained weights are stored in layers but not in the model\n",
    "    loss3_classifier_statefarm = Dense(num_classes,name='loss3/classifier',W_regularizer=l2(0.0002))(pool5_drop_7x7_s1)\n",
    "    loss3_classifier_act_statefarm = Activation('softmax',name='prob')(loss3_classifier_statefarm)\n",
    "    loss2_classifier_statefarm = Dense(num_classes,name='loss2/classifier',W_regularizer=l2(0.0002))(loss2_drop_fc)\n",
    "    loss2_classifier_act_statefarm = Activation('softmax')(loss2_classifier_statefarm)\n",
    "    loss1_classifier_statefarm = Dense(num_classes,name='loss1/classifier',W_regularizer=l2(0.0002))(loss1_drop_fc)\n",
    "    loss1_classifier_act_statefarm = Activation('softmax')(loss1_classifier_statefarm)\n",
    "\n",
    "    # Create another model with our customized softmax\n",
    "    model = Model(input=input, output=[loss1_classifier_act_statefarm,loss2_classifier_act_statefarm,loss3_classifier_act_statefarm])\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snd_data_split_3(X,Y,train_proportion,test_proportion,validation_proportion):\n",
    "    size_1 = 1 - train_proportion\n",
    "    size_2 = validation_proportion/(test_proportion+validation_proportion)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=size_1)\n",
    "    x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size=size_2)\n",
    "    return x_train,x_test,x_valid,y_train,y_test,y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 501/21302 [00:00<00:04, 5009.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate a tensor of images for train/test/validation sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21302/21302 [00:04<00:00, 4991.21it/s]\n",
      "100%|██████████| 3043/3043 [00:00<00:00, 5108.41it/s]\n",
      "100%|██████████| 6087/6087 [00:01<00:00, 5196.68it/s]\n"
     ]
    }
   ],
   "source": [
    "    img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "    channel = 3\n",
    "    num_classes = 2 \n",
    "    batch_size = 64 \n",
    "    nb_epoch = 30\n",
    "    \n",
    "    TrainDataRaw = pd.read_csv('/home/sahand/Desktop/output/log.csv',sep=',',header=None)\n",
    "    labeled_image_list = TrainDataRaw[0].unique()\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    remove_all = 205000 # Number of data points for dropping\n",
    "    drop_indices = np.random.choice(TrainDataRaw.index, remove_all, replace=False)\n",
    "    TrainDataRaw = TrainDataRaw.drop(drop_indices)\n",
    "    \n",
    "    TrainDataRaw_0 = TrainDataRaw.loc[TrainDataRaw[4]==0]\n",
    "    TrainDataRaw_1 = TrainDataRaw.loc[TrainDataRaw[4]==1]\n",
    "    TrainDataRaw_2 = TrainDataRaw.loc[TrainDataRaw[4]==2]\n",
    "    \n",
    "    TrainDataRaw_0 = TrainDataRaw_0.reset_index()\n",
    "    TrainDataRaw_0 = TrainDataRaw_0.drop(\"index\",axis=1)\n",
    "    TrainDataRaw_1 = TrainDataRaw_1.reset_index()\n",
    "    TrainDataRaw_1 = TrainDataRaw_1.drop(\"index\",axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    remove_n = 17000 # Number of non-cracks for dropping\n",
    "    drop_indices = np.random.choice(TrainDataRaw_0.index, remove_n, replace=False)\n",
    "    TrainDataRaw_0 = TrainDataRaw_0.drop(drop_indices)\n",
    "    \n",
    "    TrainDataRaw_Full = TrainDataRaw_0.append(TrainDataRaw_1)\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.append(TrainDataRaw_2)\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.reset_index()\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.drop(\"index\",axis=1)\n",
    "    \n",
    "    # SHUFFLE DATA\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.sample(frac=1).reset_index(drop=True)\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    Y_raw = TrainDataRaw_Full[4] #(5th column= cat)\n",
    "    X_files = TrainDataRaw_Full[2] #(3th column=directories)\n",
    "    x_train_raw,x_test_raw,x_valid_raw,y_train,y_test,y_valid = snd_data_split_3(X_files,Y_raw,train_proportion=0.7,test_proportion=0.1,validation_proportion=0.2)\n",
    "\n",
    "        \n",
    "    im_size = 224\n",
    "    x_train = []\n",
    "    x_valid = []\n",
    "    x_test = []\n",
    "#    y_train = []\n",
    "#    y_valid = []\n",
    "#    y_test = []\n",
    "  \n",
    "    print(\"generate a tensor of images for train/test/validation sets\")\n",
    "    \n",
    "    \n",
    "    for f in tqdm(x_train_raw):\n",
    "        img = cv2.imread(f)\n",
    "        x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "        \n",
    "    for f in tqdm(x_test_raw):\n",
    "        img = cv2.imread(f)\n",
    "        x_test.append(cv2.resize(img, (im_size, im_size)))\n",
    "        \n",
    "    for f in tqdm(x_valid_raw):\n",
    "        img = cv2.imread(f)\n",
    "        x_valid.append(cv2.resize(img, (im_size, im_size)))\n",
    "        \n",
    "\n",
    "#    for label in tqdm(y_train_r):\n",
    "#        y_train.append(label)\n",
    "        \n",
    "#    for label in tqdm(y_valid_r):\n",
    "#        y_valid.append(label)\n",
    "        \n",
    "#    for label in tqdm(y_test_r):\n",
    "#        y_test.append(label)    \n",
    "    \n",
    "    dummies_created = False\n",
    "    data_reshaped = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy labels are created\n"
     ]
    }
   ],
   "source": [
    "if dummies_created==False:\n",
    "    y_train = pd.get_dummies(y_train)\n",
    "    y_test = pd.get_dummies(y_test)\n",
    "    y_valid = pd.get_dummies(y_valid)\n",
    "    dummies_created = True\n",
    "    print('dummy labels are created')\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape\n",
      "(21302, 3, 224, 224)\n",
      "(21302, 2)\n",
      "test data shape\n",
      "(3043, 3, 224, 224)\n",
      "(3043, 2)\n",
      "valid data shape\n",
      "(6087, 3, 224, 224)\n",
      "(6087, 2)\n",
      "data reshaped and normalized\n"
     ]
    }
   ],
   "source": [
    "if data_reshaped==False:\n",
    "    y_train = np.array(y_train, np.uint8)\n",
    "    x_train = np.array(x_train, np.float32) / 255.\n",
    "    x_train = x_train.reshape(x_train.shape[0],x_train.shape[3],x_train.shape[1],x_train.shape[2])\n",
    "    print(\"train data shape\")\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    x_test  = np.array(x_test, np.float32) / 255.\n",
    "    x_test = x_test.reshape(x_test.shape[0],x_test.shape[3],x_test.shape[1],x_test.shape[2])\n",
    "    print(\"test data shape\")\n",
    "    print(x_test.shape)\n",
    "    print(y_test.shape)\n",
    "    x_valid  = np.array(x_valid, np.float32) / 255.\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0],x_valid.shape[3],x_valid.shape[1],x_valid.shape[2])\n",
    "    print(\"valid data shape\")\n",
    "    print(x_valid.shape)\n",
    "    print(y_valid.shape) \n",
    "    data_reshaped = True\n",
    "    print('data reshaped and normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (7, 7), activation=\"relu\", name=\"conv1/7x7_s2\", strides=(2, 2), padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name=\"pool1/3x3_s2\", padding=\"valid\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"conv2/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), activation=\"relu\", name=\"conv2/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name=\"pool2/3x3_s2\", padding=\"valid\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_3a/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (1, 1), activation=\"relu\", name=\"inception_3a/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"inception_3a/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\", name=\"inception_3a/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", name=\"inception_3a/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_3a/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_3a/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_3b/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_3b/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), activation=\"relu\", name=\"inception_3b/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_3b/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (5, 5), activation=\"relu\", name=\"inception_3b/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_3b/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_3b/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name=\"pool3/3x3_s2\", padding=\"valid\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (1, 1), activation=\"relu\", name=\"inception_4a/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (1, 1), activation=\"relu\", name=\"inception_4a/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(208, (3, 3), activation=\"relu\", name=\"inception_4a/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\", name=\"inception_4a/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), activation=\"relu\", name=\"inception_4a/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_4a/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_4a/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"loss1/conv\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", name=\"loss1/fc\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1000, name=\"loss1/classifier\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (1, 1), activation=\"relu\", name=\"inception_4b/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:69: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(112, (1, 1), activation=\"relu\", name=\"inception_4b/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(224, (3, 3), activation=\"relu\", name=\"inception_4b/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (1, 1), activation=\"relu\", name=\"inception_4b/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), activation=\"relu\", name=\"inception_4b/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_4b/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_4b/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_4c/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_4c/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"inception_4c/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:80: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (1, 1), activation=\"relu\", name=\"inception_4c/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:81: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), activation=\"relu\", name=\"inception_4c/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:82: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_4c/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_4c/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(112, (1, 1), activation=\"relu\", name=\"inception_4d/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(144, (1, 1), activation=\"relu\", name=\"inception_4d/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:88: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(288, (3, 3), activation=\"relu\", name=\"inception_4d/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_4d/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:90: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), activation=\"relu\", name=\"inception_4d/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:91: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_4d/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:92: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_4d/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:93: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:96: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"loss2/conv\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:98: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", name=\"loss2/fc\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:100: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1000, name=\"loss2/classifier\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), activation=\"relu\", name=\"inception_4e/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (1, 1), activation=\"relu\", name=\"inception_4e/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), activation=\"relu\", name=\"inception_4e/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:106: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_4e/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:107: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), activation=\"relu\", name=\"inception_4e/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:108: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_4e/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:109: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_4e/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:114: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name=\"pool4/3x3_s2\", padding=\"valid\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), activation=\"relu\", name=\"inception_5a/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:117: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (1, 1), activation=\"relu\", name=\"inception_5a/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:118: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), activation=\"relu\", name=\"inception_5a/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:119: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_5a/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:120: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), activation=\"relu\", name=\"inception_5a/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:121: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_5a/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:122: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_5a/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:123: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(384, (1, 1), activation=\"relu\", name=\"inception_5b/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (1, 1), activation=\"relu\", name=\"inception_5b/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:127: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(384, (3, 3), activation=\"relu\", name=\"inception_5b/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:128: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (1, 1), activation=\"relu\", name=\"inception_5b/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:129: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), activation=\"relu\", name=\"inception_5b/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:130: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_5b/pool\", padding=\"same\")`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:131: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_5b/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:132: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:137: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1000, name=\"loss3/classifier\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:141: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_1, outputs=[Softmax.0...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:149: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, name=\"loss3/classifier\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:151: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, name=\"loss2/classifier\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:153: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, name=\"loss1/classifier\", kernel_regularizer=<keras.reg...)`\n",
      "/home/sahand/anaconda3/envs/py3-theano/lib/python3.6/site-packages/ipykernel_launcher.py:157: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=/input_1, outputs=[Softmax.0...)`\n"
     ]
    }
   ],
   "source": [
    "    # Load our model\n",
    "    model = googlenet_model(img_rows, img_cols, channel, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6087, 2) (3043, 2) (21302, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_valid.shape,\n",
    "y_test.shape,\n",
    "y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21302 samples, validate on 6087 samples\n",
      "Epoch 1/30\n",
      "21302/21302 [==============================] - 163s - loss: 3.5023 - activation_4_loss: 0.6586 - activation_3_loss: 0.6966 - prob_loss: 0.6152 - activation_4_acc: 0.6944 - activation_3_acc: 0.6755 - prob_acc: 0.7039 - val_loss: 3.3372 - val_activation_4_loss: 0.6023 - val_activation_3_loss: 0.5993 - val_prob_loss: 0.6048 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 2/30\n",
      "21302/21302 [==============================] - 162s - loss: 3.3583 - activation_4_loss: 0.6106 - activation_3_loss: 0.6119 - prob_loss: 0.6061 - activation_4_acc: 0.7062 - activation_3_acc: 0.7060 - prob_acc: 0.7070 - val_loss: 3.3615 - val_activation_4_loss: 0.6093 - val_activation_3_loss: 0.6137 - val_prob_loss: 0.6086 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 3/30\n",
      "21302/21302 [==============================] - 161s - loss: 3.3564 - activation_4_loss: 0.6110 - activation_3_loss: 0.6103 - prob_loss: 0.6073 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3573 - val_activation_4_loss: 0.6097 - val_activation_3_loss: 0.6100 - val_prob_loss: 0.6115 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 4/30\n",
      "21302/21302 [==============================] - 162s - loss: 3.3480 - activation_4_loss: 0.6096 - activation_3_loss: 0.6080 - prob_loss: 0.6064 - activation_4_acc: 0.7069 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3454 - val_activation_4_loss: 0.6078 - val_activation_3_loss: 0.6079 - val_prob_loss: 0.6079 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 5/30\n",
      "21302/21302 [==============================] - 160s - loss: 3.3458 - activation_4_loss: 0.6105 - activation_3_loss: 0.6082 - prob_loss: 0.6064 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3437 - val_activation_4_loss: 0.6085 - val_activation_3_loss: 0.6075 - val_prob_loss: 0.6085 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 6/30\n",
      "21302/21302 [==============================] - 158s - loss: 3.3410 - activation_4_loss: 0.6105 - activation_3_loss: 0.6075 - prob_loss: 0.6056 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3377 - val_activation_4_loss: 0.6073 - val_activation_3_loss: 0.6071 - val_prob_loss: 0.6078 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 7/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.3336 - activation_4_loss: 0.6079 - activation_3_loss: 0.6070 - prob_loss: 0.6052 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3394 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6092 - val_prob_loss: 0.6098 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 8/30\n",
      "21302/21302 [==============================] - 158s - loss: 3.3317 - activation_4_loss: 0.6091 - activation_3_loss: 0.6066 - prob_loss: 0.6058 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3317 - val_activation_4_loss: 0.6084 - val_activation_3_loss: 0.6074 - val_prob_loss: 0.6075 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 9/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.3260 - activation_4_loss: 0.6072 - activation_3_loss: 0.6073 - prob_loss: 0.6051 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3410 - val_activation_4_loss: 0.6098 - val_activation_3_loss: 0.6144 - val_prob_loss: 0.6118 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 10/30\n",
      "21302/21302 [==============================] - 158s - loss: 3.3231 - activation_4_loss: 0.6081 - activation_3_loss: 0.6069 - prob_loss: 0.6050 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3265 - val_activation_4_loss: 0.6094 - val_activation_3_loss: 0.6076 - val_prob_loss: 0.6085 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 11/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.3165 - activation_4_loss: 0.6065 - activation_3_loss: 0.6067 - prob_loss: 0.6046 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3206 - val_activation_4_loss: 0.6080 - val_activation_3_loss: 0.6082 - val_prob_loss: 0.6076 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 12/30\n",
      "21302/21302 [==============================] - 158s - loss: 3.3116 - activation_4_loss: 0.6058 - activation_3_loss: 0.6063 - prob_loss: 0.6048 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3189 - val_activation_4_loss: 0.6109 - val_activation_3_loss: 0.6071 - val_prob_loss: 0.6080 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 13/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.3069 - activation_4_loss: 0.6054 - activation_3_loss: 0.6057 - prob_loss: 0.6050 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3174 - val_activation_4_loss: 0.6083 - val_activation_3_loss: 0.6102 - val_prob_loss: 0.6101 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 14/30\n",
      "21302/21302 [==============================] - 158s - loss: 3.3042 - activation_4_loss: 0.6060 - activation_3_loss: 0.6067 - prob_loss: 0.6047 - activation_4_acc: 0.7065 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3211 - val_activation_4_loss: 0.6113 - val_activation_3_loss: 0.6148 - val_prob_loss: 0.6098 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 15/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.3039 - activation_4_loss: 0.6065 - activation_3_loss: 0.6079 - prob_loss: 0.6064 - activation_4_acc: 0.7067 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3059 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6084 - val_prob_loss: 0.6076 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 16/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.2958 - activation_4_loss: 0.6050 - activation_3_loss: 0.6065 - prob_loss: 0.6050 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.3027 - val_activation_4_loss: 0.6087 - val_activation_3_loss: 0.6083 - val_prob_loss: 0.6083 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 17/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.2922 - activation_4_loss: 0.6049 - activation_3_loss: 0.6068 - prob_loss: 0.6050 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2975 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6076 - val_prob_loss: 0.6075 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 18/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.2869 - activation_4_loss: 0.6049 - activation_3_loss: 0.6054 - prob_loss: 0.6050 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2931 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6075 - val_prob_loss: 0.6071 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 19/30\n",
      "21302/21302 [==============================] - 159s - loss: 3.2832 - activation_4_loss: 0.6049 - activation_3_loss: 0.6058 - prob_loss: 0.6047 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2897 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6073 - val_prob_loss: 0.6077 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21302/21302 [==============================] - 157s - loss: 3.2801 - activation_4_loss: 0.6050 - activation_3_loss: 0.6062 - prob_loss: 0.6047 - activation_4_acc: 0.7066 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2880 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6086 - val_prob_loss: 0.6081 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 21/30\n",
      "21302/21302 [==============================] - 157s - loss: 3.2760 - activation_4_loss: 0.6049 - activation_3_loss: 0.6055 - prob_loss: 0.6049 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2833 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6076 - val_prob_loss: 0.6081 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 22/30\n",
      "21302/21302 [==============================] - 157s - loss: 3.2724 - activation_4_loss: 0.6049 - activation_3_loss: 0.6053 - prob_loss: 0.6051 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2797 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6075 - val_prob_loss: 0.6083 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 23/30\n",
      "21302/21302 [==============================] - 157s - loss: 3.2684 - activation_4_loss: 0.6049 - activation_3_loss: 0.6053 - prob_loss: 0.6049 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2762 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6077 - val_prob_loss: 0.6079 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 24/30\n",
      "21302/21302 [==============================] - 157s - loss: 3.2659 - activation_4_loss: 0.6049 - activation_3_loss: 0.6059 - prob_loss: 0.6052 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2736 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6086 - val_prob_loss: 0.6080 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 25/30\n",
      "21302/21302 [==============================] - 157s - loss: 3.2633 - activation_4_loss: 0.6049 - activation_3_loss: 0.6063 - prob_loss: 0.6056 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2701 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6082 - val_prob_loss: 0.6084 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 26/30\n",
      "21302/21302 [==============================] - 156s - loss: 3.2585 - activation_4_loss: 0.6049 - activation_3_loss: 0.6056 - prob_loss: 0.6052 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2667 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6085 - val_prob_loss: 0.6083 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 27/30\n",
      "21302/21302 [==============================] - 157s - loss: 3.2548 - activation_4_loss: 0.6048 - activation_3_loss: 0.6055 - prob_loss: 0.6051 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2631 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6085 - val_prob_loss: 0.6083 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 28/30\n",
      "21302/21302 [==============================] - 156s - loss: 3.2511 - activation_4_loss: 0.6049 - activation_3_loss: 0.6055 - prob_loss: 0.6051 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2598 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6087 - val_prob_loss: 0.6085 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 29/30\n",
      "21302/21302 [==============================] - 157s - loss: 3.2478 - activation_4_loss: 0.6049 - activation_3_loss: 0.6054 - prob_loss: 0.6054 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2559 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6084 - val_prob_loss: 0.6084 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n",
      "Epoch 30/30\n",
      "21302/21302 [==============================] - 156s - loss: 3.2433 - activation_4_loss: 0.6049 - activation_3_loss: 0.6050 - prob_loss: 0.6050 - activation_4_acc: 0.7070 - activation_3_acc: 0.7070 - prob_acc: 0.7070 - val_loss: 3.2525 - val_activation_4_loss: 0.6088 - val_activation_3_loss: 0.6086 - val_prob_loss: 0.6085 - val_activation_4_acc: 0.7025 - val_activation_3_acc: 0.7025 - val_prob_acc: 0.7025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3244a2f860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Start Fine-tuning. \n",
    "    # Notice that googlenet takes 3 sets of labels for outputs, one for each auxillary classifier\n",
    "    model.fit(x_train, [y_train, y_train, y_train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(x_valid, [y_valid, y_valid, y_valid])\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3043/3043 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "    # Make predictions\n",
    "    predictions_test = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Combine 3 set of outputs using averaging\n",
    "    predictions_test = sum(predictions_test)/len(predictions_test)\n",
    "\n",
    "    # Cross-entropy loss score\n",
    "    score = log_loss(y_test, predictions_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1\n",
      "0     0.697651  0.302349\n",
      "1     0.697242  0.302758\n",
      "2     0.697438  0.302562\n",
      "3     0.698073  0.301927\n",
      "4     0.697481  0.302519\n",
      "5     0.697642  0.302358\n",
      "6     0.697984  0.302016\n",
      "7     0.697550  0.302450\n",
      "8     0.697549  0.302451\n",
      "9     0.697851  0.302149\n",
      "10    0.699236  0.300764\n",
      "11    0.697688  0.302312\n",
      "12    0.697768  0.302232\n",
      "13    0.697374  0.302626\n",
      "14    0.697319  0.302681\n",
      "15    0.697510  0.302490\n",
      "16    0.697358  0.302642\n",
      "17    0.697499  0.302501\n",
      "18    0.697590  0.302410\n",
      "19    0.698570  0.301430\n",
      "20    0.697388  0.302612\n",
      "21    0.697204  0.302796\n",
      "22    0.697877  0.302123\n",
      "23    0.698050  0.301950\n",
      "24    0.697810  0.302191\n",
      "25    0.698193  0.301807\n",
      "26    0.697331  0.302669\n",
      "27    0.697266  0.302734\n",
      "28    0.697864  0.302136\n",
      "29    0.697562  0.302438\n",
      "...        ...       ...\n",
      "3013  0.697676  0.302324\n",
      "3014  0.697606  0.302394\n",
      "3015  0.697083  0.302917\n",
      "3016  0.698095  0.301905\n",
      "3017  0.697819  0.302182\n",
      "3018  0.697040  0.302960\n",
      "3019  0.698395  0.301605\n",
      "3020  0.697535  0.302465\n",
      "3021  0.698079  0.301921\n",
      "3022  0.697454  0.302546\n",
      "3023  0.697873  0.302127\n",
      "3024  0.697964  0.302036\n",
      "3025  0.697691  0.302309\n",
      "3026  0.697999  0.302001\n",
      "3027  0.697604  0.302396\n",
      "3028  0.698023  0.301977\n",
      "3029  0.697899  0.302101\n",
      "3030  0.697882  0.302118\n",
      "3031  0.697224  0.302776\n",
      "3032  0.697589  0.302411\n",
      "3033  0.697921  0.302079\n",
      "3034  0.697454  0.302546\n",
      "3035  0.697222  0.302778\n",
      "3036  0.698051  0.301949\n",
      "3037  0.697241  0.302759\n",
      "3038  0.697507  0.302493\n",
      "3039  0.697353  0.302647\n",
      "3040  0.698335  0.301665\n",
      "3041  0.697800  0.302200\n",
      "3042  0.698152  0.301848\n",
      "\n",
      "[3043 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "y_test_comparision = pd.DataFrame(y_test)\n",
    "y_pred_comparison = pd.DataFrame(predictions_test)\n",
    "\n",
    "print(y_pred_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion = pd.crosstab(y_pred_comparison, y_test_comparision, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Example to fine-tune on 3000 samples from Cifar10\n",
    "\n",
    "    img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "    channel = 3\n",
    "    num_classes = 2 \n",
    "    batch_size = 16 \n",
    "    nb_epoch = 10\n",
    "    \n",
    "    TrainDataRaw = pd.read_csv('/home/sahand/Desktop/output/log.cvs',sep=',',header=None)\n",
    "    labeled_image_list = TrainDataRaw[0].unique()\n",
    "    np.random.seed(10)\n",
    "    \n",
    "    remove_all = 0 # Number of non-cracks for dropping\n",
    "    drop_indices = np.random.choice(TrainDataRaw.index, remove_all, replace=False)\n",
    "    TrainDataRaw = TrainDataRaw.drop(drop_indices)\n",
    "    \n",
    "    TrainDataRaw_0 = TrainDataRaw.loc[TrainDataRaw[4]==0]\n",
    "    TrainDataRaw_1 = TrainDataRaw.loc[TrainDataRaw[4]==1]\n",
    "    TrainDataRaw_2 = TrainDataRaw.loc[TrainDataRaw[4]==2]\n",
    "    \n",
    "    TrainDataRaw_0 = TrainDataRaw_0.reset_index()\n",
    "    TrainDataRaw_0 = TrainDataRaw_0.drop(\"index\",axis=1)\n",
    "    TrainDataRaw_1 = TrainDataRaw_1.reset_index()\n",
    "    TrainDataRaw_1 = TrainDataRaw_1.drop(\"index\",axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    remove_n = 1 # Number of non-cracks for dropping\n",
    "    drop_indices = np.random.choice(TrainDataRaw_0.index, remove_n, replace=False)\n",
    "    TrainDataRaw_0 = TrainDataRaw_0.drop(drop_indices)\n",
    "    \n",
    "    TrainDataRaw_Full = TrainDataRaw_0.append(TrainDataRaw_1)\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.append(TrainDataRaw_2)\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.reset_index()\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.drop(\"index\",axis=1)\n",
    "    \n",
    "    # SHUFFLE DATA\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.sample(frac=1).reset_index(drop=True)\n",
    "    TrainDataRaw_Full = TrainDataRaw_Full.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    Y_raw = TrainDataRaw_Full[4] #(5th column= cat)\n",
    "    X_files = TrainDataRaw_Full[2] #(3th column=directories)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=size_1)\n",
    "    \n",
    "    im_size = 224\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "  \n",
    "    #generate a tensor of images\n",
    "    \n",
    "    i = 0\n",
    "    for f, breed in tqdm(df_train.values):\n",
    "        img = cv2.imread(f)\n",
    "        label = one_hot_labels[i]\n",
    "        x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "        y_train.append(label)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for f in tqdm(df_test['id'].values):\n",
    "        img = cv2.imread(directory_path+'test/{}.jpg'.format(f))\n",
    "        x_test.append(cv2.resize(img, (im_size, im_size)))\n",
    "\n",
    "    y_train_raw = np.array(y_train, np.uint8)\n",
    "    x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "    x_test  = np.array(x_test, np.float32) / 255.\n",
    "\n",
    "    print(x_train_raw.shape)\n",
    "    print(y_train_raw.shape)\n",
    "    print(x_test.shape)\n",
    "\n",
    "    num_class = y_train_raw.shape[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Load Cifar10 data. Please implement your own load_data() module for your own dataset\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.1, random_state=1)\n",
    "\n",
    "    # Load our model\n",
    "    model = googlenet_model(img_rows, img_cols, channel, num_classes)\n",
    "\n",
    "    # Start Fine-tuning. \n",
    "    # Notice that googlenet takes 3 sets of labels for outputs, one for each auxillary classifier\n",
    "    model.fit(X_train, [Y_train, Y_train, Y_train],\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(X_valid, [Y_valid, Y_valid, Y_valid]),\n",
    "              )\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    # Combine 3 set of outputs using averaging\n",
    "    predictions_valid = sum(predictions_valid)/len(predictions_valid)\n",
    "\n",
    "    # Cross-entropy loss score\n",
    "    score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
